import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import metrics
from numpy import unique
from numpy import argmax
from keras_tuner import RandomSearch
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.datasets.mnist import load_data
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from kedro.extras.datasets.tensorflow import TensorFlowModelDataset

def list_fp(input_pickle):
    column_list = list(input_pickle.columns[6:12])
    return column_list

def train_test_split_column(input_pickle, column):
    X = input_pickle[column]
    y = input_pickle['Label'] #Label drug_class_code
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    return X_train, X_test, y_train, y_test

def build_selection_model(inshape, nclasses):
    """
    Function that builds a CNN model with non optimal conditions.
    Input: tuple with inner shape of arrays, integer with number of classes
    Output: a CNN model
    """
    model = Sequential()
    model.add(Conv1D(100, 9, activation='relu', kernel_initializer='he_uniform', input_shape=inshape))
    model.add(MaxPool1D(2))
    model.add(Flatten())
    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.5))
    model.add(Dense(nclasses, activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

def train_test_split_columns(input_pickle, column_list):
    """
    Function that takes a list of columns and makes a train/test split for each of them with the column containing the labels as y.
    Then returns a dictionary with the column name as a key and the tuple with X_train, X_test, y_train, y_test as value.
    Input: list with column names
    Output: dictionary
    """
    splits_dic = {}
    for column in column_list:
        X_train, X_test, y_train, y_test = train_test_split_column(input_pickle, column)
        splits_dic[column] = X_train, X_test, y_train, y_test
    return splits_dic

def reshape_input(tuple_split):
    X_train = np.array(list(tuple_split[0]))
    print('Shape X_train: ', X_train.shape, type(X_train))
    X_test = np.array(list(tuple_split[1]))
    n_classes = len(np.unique(tuple_split[2]))
    print('Number of classes: ', n_classes)
    X_train= X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
    X_test= X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
    print('Reshaped X_train: ', X_train.shape)
    in_shape = X_train.shape[1:]
    print('In_shape: ', in_shape)
    print('Shape y_train: ', tuple_split[2].shape)
    return X_train, X_test, in_shape, n_classes

def build_array_dic(splits_dic):
    """
    Function that takes the dictionary generated by "train_test_split_columns" and reshapes X_train to fit a CNN model.
    Then builds the CNN model using the function 'build_selection_model'. Return both the reshaped arrays and the models.
    Input: dictionary with column names as keys and a tuple of arrays as values
    Output: dictionary with column names as keys and a tuple with both reshaped arrays and CNN models 
    """
    arrays_models_dic = {}
    for column, tup in splits_dic.items():                                                                                                                                                                                                                                                                              
        model_data = reshape_input(tup)
        
        model = build_selection_model(model_data[2], model_data[3])
        arrays_models_dic[column] = model_data[0], model_data[1], tup[2], tup[3], model

    return arrays_models_dic

def fit_selection_model(arrays_models_dic):
    """
    Function that takes the dictionary generated by 'reshape_and_build' and fits the CNN model contained in the values with the X_train and y_train arrays also in the values.
    Then evaluates the model against X_test e y_test and returns the accuracy of each model.
    Input: dictionary with column names as keys and a tuple with reshaped arrays and CNN models
    Output: a dictionary with column names as keys and the accuracy obtained by the CNN model for that column
    """
    accuracies_dic = {}
    es = EarlyStopping(monitor='val_loss', patience=1)
    for column, tup in arrays_models_dic.items():
        print(f"Analysing {column}")
        tup[4].fit(tup[0], tup[2], epochs=10, batch_size=128, verbose=1, validation_split = 0.2, callbacks = [es])
        loss, acc = tup[4].evaluate(tup[1], tup[3], verbose=1)
        accuracies_dic[column] = f'{acc:.3f}'
    return accuracies_dic

def choose_fingerprints(fingerprints_accuracies):
    return max(fingerprints_accuracies, key=fingerprints_accuracies.get)

def tune_hp_def_model(model_data, y_train, y_test):
    print('X_train shape: ', model_data[0].shape)
    print('y_train shape: ', y_train.shape)
    print('X_test shape: ', model_data[1].shape)
    print('X_test shape: ', y_test.shape)
    tuner = RandomSearch(build_def_model,
                    objective='val_loss',
                    max_trials = 5,
                    directory = os.path.join('tempHERE', 'tuner', 'RS_tuned_model', '230714_09'))
    tuner.search(model_data[0],y_train,epochs=3,validation_data=(model_data[1],y_test))
    tuned_model = tuner.get_best_models(num_models=1)[0]
    tuned_model.summary()
    
    return tuned_model

def train_def_model(tuned_model, model_data, y_train, y_test):
    # Configure early stopping
    es = EarlyStopping(monitor='val_loss', patience=10)
    mc = ModelCheckpoint(filepath = os.path.join('compiled_modelsHERE','checkpoints', '{epoch:02d}-{val_accuracy:.3f}.hdf5'), monitor = 'val_loss', save_best_only = True)
    # Fit the model
    history = tuned_model.fit(model_data[0], y_train, epochs=200, batch_size=128, verbose=1, validation_split = 0.3, callbacks = [es,mc])
    # Evaluate the model
    loss, acc = tuned_model.evaluate(model_data[1], y_test, verbose=1)
    """ data_set = TensorFlowModelDataset('C:/Users/josin/AppData/Local/Temp/kedro_tensorflow_tmpne6d4sgb/saved_model.pb')
    data_set.save(tuned_model) """
    return tuned_model, history

def build_def_model(hp):
    """
    Function that chooses the best hyperparameters for a CNN model and then compiles it.
    Input: a set of hyperparamneters
    Output: a cnn model
    """
    # Create model object
    model = keras.Sequential()
    # Choose number of layers
    for i in range(hp.Int("num_layers", 1, 5)):
        model.add(
            layers.Conv1D(
        filters=hp.Int('conv_1_filter', min_value=16, max_value=128, step=16),
        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),
        activation='relu',
        input_shape=(2048, 1),
        padding='valid')) #no padding
        model.add(layers.MaxPool1D(hp.Int('pool_size', min_value=2, max_value=6)))
        if hp.Boolean("dropout"):
            model.add(layers.Dropout(rate=0.25))

    model.add(layers.Flatten())
    model.add(layers.Dense(
        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),
        activation='relu', kernel_initializer = 'he_uniform'
    ))

    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(16, activation='softmax'))

    # Compilation of model
    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
    return model

def get_predictions(tuned_model, x_test, y_test):
    y_pred = tuned_model.predict(x_test)
    y_pred_list = [argmax(x) for x in y_pred]
    print(metrics.classification_report(y_test,y_pred_list))
    return y_pred_list

def save_predictions(y_pred_list):
    return pd.Series(y_pred_list)

def visualize_training(history):
    fig,ax = plt.subplots()
    plt.style.use('ggplot')

    epochs = len(history.history['loss'])
    epoch_values = list(range(epochs))

    ax.plot(epoch_values, history.history['loss'], label='Training loss')
    ax.plot(epoch_values, history.history['val_loss'], label='Validation loss')
    ax.plot(epoch_values, history.history['accuracy'], label='Training accuracy')
    ax.plot(epoch_values, history.history['val_accuracy'], label='Validation accuracy')

    ax.set_title('Training loss and accuracy')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss/Accuracy')
    ax.legend()
    plt.show()

def determine_best_fp(input_pickle):
    column_list = list_fp(input_pickle)
    splits_dic = train_test_split_columns(input_pickle, column_list)
    array_model_dic= build_array_dic(splits_dic)
    accuracies_dic = fit_selection_model(array_model_dic)
    return accuracies_dic

def prepare_data(input_pickle, fingerprints_accuracies):
    selected_fp = max(fingerprints_accuracies, key=fingerprints_accuracies.get)
    split_col = train_test_split_column(input_pickle, selected_fp)
    model_data = reshape_input(split_col)
    return split_col, model_data, selected_fp

def obtain_model(split_col,model_data):
    tuned_model = tune_hp_def_model(model_data, split_col[2], split_col[3])
    def_model, history = train_def_model(tuned_model, model_data, split_col[2], split_col[3])
    y_pred_list = get_predictions(tuned_model, model_data[1], split_col[3])
    predictions = save_predictions(y_pred_list)
    #visualize_training(history)
    return def_model, predictions

""" def evaluate_and_visualize(def_model, split_col, model_data, history):
    cnn_model = load_model(def_model)
    y_pred_list = get_predictions(cnn_model, model_data[1], split_col[3])
    predictions = save_predictions(y_pred_list)
    visualize_training(history)
    return predictions """
